{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoVLe3PmRXMZSvAfoiXimV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GithubofRuZhang/Robust-Princing-Optimization/blob/main/Robust_Quadratic_Programming_for_Price_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\begin{array}{l}\\qquad g(x, \\gamma):=v(x)^{\\top}\\left(\\hat{Q}+\\lambda \\frac{\\gamma M_{1}+M_{2} / \\gamma}{2}\\right) v(x) . \\\\ \\text { where } \\\\ \\qquad M_{1}:=L_{1}^{\\top} L_{1}, \\quad M_{2}:=L_{2}^{\\top} L_{2} .\\end{array} $"
      ],
      "metadata": {
        "id": "uF3TgJCTQdOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function $ h:(0, \\infty) \\rightarrow \\mathbf{R} $ by\n",
        "$$\n",
        "h(\\gamma):=\\min _{x \\in \\mathcal{X}} g(x, \\gamma)\n",
        "$$"
      ],
      "metadata": {
        "id": "ahIhniOVQk5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algorithm 1 Golden Section Search\n",
        "\n",
        "Require: $ \\hat{Q}, L_{1}, L_{2}, \\lambda, \\alpha, \\beta, \\delta $\n",
        "\n",
        "Initialize $ a=\\alpha, b=\\beta, r=(\\sqrt{5}-1) / 2 $\n",
        "\n",
        "while $ |a-b| \\geq \\delta $ do\n",
        "\n",
        "$ c \\leftarrow b-r *(b-a), d \\leftarrow a+r *(b-a) $\n",
        "\n",
        "$ b \\leftarrow d $ if $ h(c)<h(d) $, and $ a \\leftarrow c $ otherwise.\n",
        "end while\n",
        "\n",
        "Output $ \\tilde{x}:= oracle(x, \\tilde{\\gamma})=\\arg \\min _{x \\in \\mathcal{X}} g(x, \\tilde{\\gamma}) $ where $ \\tilde{\\gamma}=(a+b) / 2 $."
      ],
      "metadata": {
        "id": "vS5FMB_hQSnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assume parameters and functions\n",
        "# Here are just examples, the actual implementation needs to be defined based on the specific parameters and constraints of the problem\n",
        "\n",
        "# Example parameters\n",
        "M = 3  # Assume there are 3 products, can be adjusted as needed\n",
        "Q_hat = np.random.rand(M, M)  # Assumed Q_hat\n",
        "L1 = np.random.rand(M, M)  # Assumed L1\n",
        "L2 = np.random.rand(M, M)  # Assumed L2\n",
        "lambda_ = 1  # Assumed lambda\n",
        "alpha = 0.1  # Initial lower bound of the search interval\n",
        "beta = 2  # Initial upper bound of the search interval\n",
        "delta = 0.001  # Accuracy requirement\n",
        "\n",
        "# Define the g(x, gamma) function\n",
        "def g(x, gamma):\n",
        "    M1 = L1.T @ L1\n",
        "    M2 = L2.T @ L2\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2\n",
        "    return x.T @ term @ x\n",
        "\n",
        "# Define the h(gamma) function, here simplified to use a fixed x value\n",
        "def h(gamma):\n",
        "    x = np.array([1] * M)  # Assumed x value, in reality, it should be the solution to the optimization problem\n",
        "    return g(x, gamma)\n",
        "\n",
        "# Golden section search algorithm\n",
        "def golden_section_search(alpha, beta, delta):\n",
        "    r = (np.sqrt(5) - 1) / 2\n",
        "    a, b = alpha, beta\n",
        "\n",
        "    while abs(b - a) >= delta:\n",
        "        c = b - r * (b - a)\n",
        "        d = a + r * (b - a)\n",
        "        if h(c) < h(d):\n",
        "            b = d\n",
        "        else:\n",
        "            a = c\n",
        "\n",
        "    return (a + b) / 2\n",
        "\n",
        "# Execute the algorithm\n",
        "gamma_tilde = golden_section_search(alpha, beta, delta)\n",
        "gamma_tilde\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmPkwCx2cRYp",
        "outputId": "1ffc7b4c-4b98-48cf-f6d6-bb425a9951ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1174273486697541"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 假设参数和函数\n",
        "# 这里仅为示例，具体实现需要根据问题的实际参数和约束来定义\n",
        "\n",
        "# 示例参数\n",
        "M = 3  # 假设有3个产品，可以根据需要调整\n",
        "Q_hat = np.random.rand(M, M)# 假设的 Q_hat\n",
        "L1 = np.random.rand(M, M) # 假设的 L1\n",
        "L2 = np.random.rand(M, M)  # 假设的 L2\n",
        "lambda_ = 1  # 假设的 lambda\n",
        "alpha = 0.1  # 初始搜索区间下限\n",
        "beta = 2  # 初始搜索区间上限\n",
        "delta = 0.001  # 精度要求\n",
        "\n",
        "# 定义 g(x, gamma) 函数\n",
        "def g(x, gamma):\n",
        "    M1 = L1.T @ L1\n",
        "    M2 = L2.T @ L2\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2\n",
        "    return x.T @ term @ x\n",
        "\n",
        "# 定义 h(gamma) 函数，这里简化为使用固定的 x 值\n",
        "def h(gamma):\n",
        "    x = np.array([1] * M)  # 假设的 x 值，实际情况中应该是优化问题的解\n",
        "    return g(x, gamma)\n",
        "\n",
        "# 黄金分割搜索算法\n",
        "def golden_section_search(alpha, beta, delta):\n",
        "    r = (np.sqrt(5) - 1) / 2\n",
        "    a, b = alpha, beta\n",
        "\n",
        "    while abs(b - a) >= delta:\n",
        "        c = b - r * (b - a)\n",
        "        d = a + r * (b - a)\n",
        "        if h(c) < h(d):\n",
        "            b = d\n",
        "        else:\n",
        "            a = c\n",
        "\n",
        "    return (a + b) / 2\n",
        "\n",
        "# 执行算法\n",
        "gamma_tilde = golden_section_search(alpha, beta, delta)\n",
        "gamma_tilde\n"
      ],
      "metadata": {
        "id": "nL-pScKzbkZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ccc8c7-850c-4b43-c8f1-c70d993d91fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6407877624044351"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the algorithm to support arbitrary dimensions of x\n",
        "# Assume parameters and functions are updated\n",
        "M = 3  # Assume there are 3 products, can be adjusted as needed\n",
        "Q_hat = np.random.rand(M, M)  # Assumed Q_hat\n",
        "L1 = np.random.rand(M, M)  # Assumed L1\n",
        "L2 = np.random.rand(M, M)  # Assumed L2\n",
        "lambda_ = 1  # Assumed lambda\n",
        "alpha = 0.1  # Initial lower bound of the search interval\n",
        "beta = 2  # Initial upper bound of the search interval\n",
        "delta = 0.001  # Accuracy requirement\n",
        "x_options = np.array([0.6, 0.7, 0.8, 0.9, 1.0])  # Simplified x choices\n",
        "\n",
        "x_options = np.array([0.6, 0.7, 0.8, 0.9, 1.0])  # Simplified x choices\n",
        "\n",
        "# Update the Oracle algorithm to support M-dimensional x\n",
        "def oracle(gamma, M, x_options):\n",
        "    min_val = np.inf\n",
        "    x_opt = None\n",
        "\n",
        "    # Generate all possible combinations of x\n",
        "    X = np.array(np.meshgrid(*[x_options for _ in range(M)])).T.reshape(-1, M)\n",
        "\n",
        "    # Iterate over all combinations of x\n",
        "    for x in X:\n",
        "        val = g(x, gamma)\n",
        "        if val < min_val:\n",
        "            min_val = val\n",
        "            x_opt = x\n",
        "\n",
        "    return x_opt\n",
        "\n",
        "# Execute the golden section search using the updated Oracle algorithm\n",
        "gamma_tilde = golden_section_search(alpha, beta, delta)\n",
        "x_tilde = oracle(gamma_tilde, M, x_options)\n",
        "\n",
        "gamma_tilde, x_tilde\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F37NA93dcc2Q",
        "outputId": "9cac0195-a07c-4445-b43b-6dad5829d723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.218270938964774, array([0.6, 0.6, 0.6]))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 更新算法以支持任意维度的x\n",
        "# 假设参数和函数更新\n",
        "M = 3  # 假设有3个产品，可以根据需要调整\n",
        "Q_hat = np.random.rand(M, M)# 假设的 Q_hat\n",
        "L1 = np.random.rand(M, M) # 假设的 L1\n",
        "L2 = np.random.rand(M, M)  # 假设的 L2\n",
        "lambda_ = 1  # 假设的 lambda\n",
        "alpha = 0.1  # 初始搜索区间下限\n",
        "beta = 2  # 初始搜索区间上限\n",
        "delta = 0.001  # 精度要求\n",
        "x_options = np.array([0.6, 0.7, 0.8, 0.9, 1.0])  # 简化的 x 选择\n",
        "\n",
        "x_options = np.array([0.6, 0.7, 0.8, 0.9, 1.0])  # 简化的 x 选择\n",
        "\n",
        "# 更新 Oracle 算法以支持 M 维 x\n",
        "def oracle(gamma, M, x_options):\n",
        "    min_val = np.inf\n",
        "    x_opt = None\n",
        "\n",
        "    # 生成所有可能的x组合\n",
        "    X = np.array(np.meshgrid(*[x_options for _ in range(M)])).T.reshape(-1, M)\n",
        "\n",
        "    # 遍历所有x组合\n",
        "    for x in X:\n",
        "        val = g(x, gamma)\n",
        "        if val < min_val:\n",
        "            min_val = val\n",
        "            x_opt = x\n",
        "\n",
        "    return x_opt\n",
        "\n",
        "# 使用更新的 Oracle 算法执行黄金分割搜索\n",
        "gamma_tilde = golden_section_search(alpha, beta, delta)\n",
        "x_tilde = oracle(gamma_tilde, M, x_options)\n",
        "\n",
        "gamma_tilde, x_tilde\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlUcqv1-C_9L",
        "outputId": "b2b475b5-b366-42d7-a48c-4ead8bf80c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7817463680623973, array([0.6, 0.6, 0.6]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Updated parameter definitions\n",
        "# Define assumed parameters\n",
        "M = 3  # Dimension\n",
        "Q_hat = np.random.rand(M, M)  # Assumed Q_hat\n",
        "L1 = np.random.rand(M, M)  # Assumed L1\n",
        "L2 = np.random.rand(M, M)  # Assumed L2\n",
        "lambda_ = 1  # Assumed lambda\n",
        "alpha = 0.1\n",
        "beta = 2\n",
        "delta = 0.001\n",
        "\n",
        "# Redefine g(x, gamma) to match the updated parameters\n",
        "def g(x, gamma):\n",
        "    M1 = L1.T @ L1\n",
        "    M2 = L2.T @ L2\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2\n",
        "    # print(M1)\n",
        "    # print(M2)\n",
        "    # print(term)\n",
        "    # print(x)\n",
        "    return x.T @ term @ x\n",
        "\n",
        "# Other functions remain unchanged\n",
        "\n",
        "# Execute golden section search and use the Oracle algorithm\n",
        "gamma_tilde = golden_section_search(alpha, beta, delta)\n",
        "x_tilde = oracle(gamma_tilde, M, x_options)\n",
        "\n",
        "gamma_tilde, x_tilde\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GtlyQIycn6r",
        "outputId": "bf50563e-887b-43ae-eaf0-a1cda1b41fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0261312559819529, array([0.6, 0.6, 0.6]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 更新的参数定义\n",
        "# 定义假设的参数\n",
        "M = 3  # 维度\n",
        "Q_hat = np.random.rand(M, M)# 假设的 Q_hat\n",
        "L1 = np.random.rand(M, M) # 假设的 L1\n",
        "L2 = np.random.rand(M, M)  # 假设的 L2\n",
        "lambda_ = 1  # 假设的 lambda\n",
        "lambda_ = 1\n",
        "alpha = 0.1\n",
        "beta = 2\n",
        "delta = 0.001\n",
        "\n",
        "\n",
        "# 重新定义g(x, gamma)以匹配更新的参数\n",
        "def g(x, gamma):\n",
        "    M1 = L1.T @ L1\n",
        "    M2 = L2.T @ L2\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2\n",
        "    # print(M1)\n",
        "    # print(M2)\n",
        "    # print(term)\n",
        "    # print(x)\n",
        "    return x.T @ term @ x\n",
        "\n",
        "# 其他函数保持不变\n",
        "\n",
        "# 执行黄金分割搜索并使用Oracle算法\n",
        "gamma_tilde = golden_section_search(alpha, beta, delta)\n",
        "x_tilde = oracle(gamma_tilde, M, x_options)\n",
        "\n",
        "gamma_tilde, x_tilde\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1q-prpOGvgE",
        "outputId": "80dff69d-50cd-4a0f-a586-735424133b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.5760360673325113, array([0.6, 0.6, 0.6]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve the optimization problem over a continuous interval for the given function $ g(x, \\gamma) $, $ \\tilde{x}:=\\arg \\min _{x \\in \\mathcal{X}} g(x, \\tilde{\\gamma}) $, we can use numerical optimization methods. For this problem, a simple yet effective approach is to use gradient descent, although it requires computing the gradient of $ g(x, \\gamma) $ with respect to $ x $.\n",
        "\n",
        "Let $ x $ be an $ M $-dimensional vector, and assume the gradients of $ g(x, \\gamma) $ with respect to each element $ x_{i} $ can be computed. In practice, if $ g(x, \\gamma) $ is a smooth function, we can obtain these gradients by differentiation.\n",
        "\n",
        "### Simplified Gradient Descent Algorithm\n",
        "Gradient descent is an iterative algorithm that seeks the minimum value by updating variables in the direction of the negative gradient of the objective function at each step. For $ g(x, \\gamma) $, the update formula can be written as:\n",
        "$$\n",
        "x^{(k+1)}=x^{(k)}-\\alpha \\nabla_{x} g\\left(x^{(k)}, \\gamma\\right)\n",
        "$$\n",
        "\n",
        "where $ x^{(k)} $ is the value of $ x $ at step $ k $, $ \\alpha $ is the learning rate, a small positive number, and $ \\nabla_{x} g\\left(x^{(k)}, \\gamma\\right) $ is the gradient of $ g(x, \\gamma) $ with respect to $ x $ at $ x^{(k)} $.\n",
        "\n",
        "### Algorithm Steps\n",
        "1. **Initialization**: Choose an initial point $ x^{(0)} $, set the learning rate $ \\alpha $, and tolerance $ \\epsilon $.\n",
        "2. **Iterative Update**:\n",
        "   - Compute the gradient: $ \\nabla_{x} g\\left(x^{(k)}, \\tilde{\\gamma}\\right) $.\n",
        "   - Update $ x: x^{(k+1)}=x^{(k)}-\\alpha \\nabla_{x} g\\left(x^{(k)}, \\tilde{\\gamma}\\right) $.\n",
        "3. **Termination Condition**: Stop iteration when $ \\left\\|x^{(k+1)}-x^{(k)}\\right\\|<\\epsilon $.\n",
        "\n",
        "### Example Implementation\n",
        "Below is a simplified example demonstrating how to implement this process. Please note that we need to compute the gradient based on the specific form of $ g(x, \\gamma) $.\n"
      ],
      "metadata": {
        "id": "r0b1M0I7cxXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "要解决给定的 $ g(x, \\gamma) $ 函数在连续区间上的优化问题 $ \\tilde{x}:=\\arg \\min _{x \\in \\mathcal{X}} g(x, \\tilde{\\gamma}) $ ，我们可以使用数值优化方法。对于这个问题，一个简单但有效的方法是使用梯度下降法，尽管需要计算 $ g(x, \\gamma) $ 关于 $ x $ 的梯度。\n",
        "\n",
        "假设 $ x $ 是一个 $ M $ 维向量，并且 $ g(x, \\gamma) $ 对每个元素 $ x_{i} $ 的梯度可以计算。在实际情况中，如果 $ g(x, \\gamma) $ 是一个光滑函数，我们可以通过求导来获得这些梯度。\n",
        "\n",
        "简化的梯度下降法\n",
        "梯度下降法是一种迭代算法，通过在每一步中沿着目标函数的负梯度方向更新变量来寻找最小值。对于 $ g(x, \\gamma) $ ，更新公式可以写为:\n",
        "$$\n",
        "x^{(k+1)}=x^{(k)}-\\alpha \\nabla_{x} g\\left(x^{(k)}, \\gamma\\right)\n",
        "$$\n",
        "\n",
        "其中， $ x^{(k)} $ 是第 $ k $ 步的 $ x $ 值， $ \\alpha $ 是学习率，一个小的正数， $ \\nabla_{x} g\\left(x^{(k)}, \\gamma\\right) $ 是 $ g(x, \\gamma) $在 $ x^{(k)} $ 处对 $ x $ 的梯度。\n",
        "\n",
        "算法步骤\n",
        "1. 初始化: 选择一个初始点 $ x^{(0)} $ ，设置学习率 $ \\alpha $ 和容忍度 $ \\epsilon $ 。\n",
        "2. 迭代更新:\n",
        "- 计算梯度: $ \\nabla_{x} g\\left(x^{(k)}, \\tilde{\\gamma}\\right) $ 。\n",
        "- 更新 $ x: x^{(k+1)}=x^{(k)}-\\alpha \\nabla_{x} g\\left(x^{(k)}, \\tilde{\\gamma}\\right) $ 。\n",
        "3. 终止条件: 当 $ \\left\\|x^{(k+1)}-x^{(k)}\\right\\|<\\epsilon $ 时停止迭代。\n",
        "\n",
        "示例实现\n",
        "下面是一个简化的示例，展示如何实现这一过程。请注意，我们需要根据 $ g(x, \\gamma) $ 的具体形式来计算梯度。"
      ],
      "metadata": {
        "id": "QG7Efo4pLBCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_g(x, gamma):\n",
        "    # Gradient calculation based on the specific form of g(x, gamma)\n",
        "    # Example gradient calculation, for reference only\n",
        "    grad = 2 * (Q_hat + lambda_ * (gamma * L1.T @ L1 + (L2.T @ L2) / gamma) / 2) @ x\n",
        "    return grad\n",
        "\n",
        "def gradient_descent(x_init, gamma, alpha, epsilon, max_iter=1000):\n",
        "    x = x_init\n",
        "    for i in range(max_iter):\n",
        "        grad = gradient_g(x, gamma)\n",
        "        x_new = x - alpha * grad\n",
        "        if np.linalg.norm(x_new - x) < epsilon:\n",
        "            break\n",
        "        x = x_new\n",
        "    return x\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Generate random numbers within a specified range, for example, within the [0.6, 1.0] interval\n",
        "x_init = np.random.rand(M) * 0.2 + 0.8\n",
        "gamma = gamma_tilde  # Previously computed gamma_tilde\n",
        "alpha = 0.0000001  # Learning rate\n",
        "epsilon = 1e-3  # Tolerance\n",
        "\n",
        "x_opt = gradient_descent(x_init, gamma, alpha, epsilon)\n",
        "x_opt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5DDdB3Pc70F",
        "outputId": "694e72fe-0adf-42b8-f471-d2ec63171e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.80082512, 0.97349224, 0.80629536])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_g(x, gamma):\n",
        "    # 这里需要根据g(x, gamma)的具体形式来计算梯度\n",
        "    # 示例梯度计算，仅供参考\n",
        "    grad = 2 * (Q_hat + lambda_ * (gamma * L1.T @ L1 + (L2.T @ L2) / gamma) / 2) @ x\n",
        "    return grad\n",
        "\n",
        "def gradient_descent(x_init, gamma, alpha, epsilon, max_iter=1000):\n",
        "    x = x_init\n",
        "    for i in range(max_iter):\n",
        "        grad = gradient_g(x, gamma)\n",
        "        x_new = x - alpha * grad\n",
        "        if np.linalg.norm(x_new - x) < epsilon:\n",
        "            break\n",
        "        x = x_new\n",
        "    return x\n",
        "\n",
        "# 使用示例\n",
        "\n",
        "# 生成在指定范围内的随机数，例如在[0.6, 1.0]区间内\n",
        "x_init = np.random.rand(M) * 0.2 + 0.8\n",
        "gamma = gamma_tilde  # 之前计算得到的gamma_tilde\n",
        "alpha = 0.0000001  # 学习率\n",
        "epsilon = 1e-3  # 容忍度\n",
        "\n",
        "x_opt = gradient_descent(x_init, gamma, alpha, epsilon)\n",
        "x_opt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVWuhIDGDPn8",
        "outputId": "fee9993b-e7c4-4dfa-f3f1-fe2c7b3020cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.92965874, 0.99867665, 0.98175512])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the gradient computation function\n",
        "def gradient_g(x, gamma):\n",
        "    # Compute the gradient based on the actual expression of g(x, gamma)\n",
        "    # Here, the assumed parameters from the example are used for calculation\n",
        "    M1 = L1.T @ L1\n",
        "    M2 = L2.T @ L2\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2\n",
        "    grad = 2 * (term.T + term) @ x\n",
        "    return grad\n",
        "\n",
        "# Define the gradient descent method\n",
        "def gradient_descent(x_init, gamma, alpha, epsilon, max_iter=1000):\n",
        "    x = x_init\n",
        "    for i in range(max_iter):\n",
        "        grad = gradient_g(x, gamma)\n",
        "        x_new = x - alpha * grad\n",
        "        if np.linalg.norm(x_new - x) < epsilon:\n",
        "            break\n",
        "        x = x_new\n",
        "    return x\n",
        "\n",
        "# Initialize parameters\n",
        "x_init = np.random.rand(M) * 60 + 100  # Random initialization for x\n",
        "print(x_init)\n",
        "gamma = 0.9997  # Use the previously computed gamma_tilde\n",
        "alpha = 0.0000001  # Learning rate\n",
        "epsilon = 1e-6  # Tolerance\n",
        "\n",
        "# Execute gradient descent algorithm to find the optimal x\n",
        "x_opt = gradient_descent(x_init, gamma, alpha, epsilon)\n",
        "\n",
        "x_opt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_8mBOgEdCrH",
        "outputId": "2d13e44e-d4f0-4772-ef99-eefda4adb8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[114.77332052 110.53089976 121.8014363 ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([114.61645384, 110.35496652, 121.63487912])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义梯度计算函数\n",
        "def gradient_g(x, gamma):\n",
        "    # 根据g(x, gamma)的实际表达式计算梯度\n",
        "    # 这里使用示例中的假设参数进行计算\n",
        "    M1 = L1.T @ L1\n",
        "    M2 = L2.T @ L2\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2\n",
        "    grad = 2 * (term.T+term) @ x\n",
        "    return grad\n",
        "\n",
        "# 定义梯度下降法\n",
        "def gradient_descent(x_init, gamma, alpha, epsilon, max_iter=1000):\n",
        "    x = x_init\n",
        "    for i in range(max_iter):\n",
        "        grad = gradient_g(x, gamma)\n",
        "        x_new = x - alpha * grad\n",
        "        if np.linalg.norm(x_new - x) < epsilon:\n",
        "            break\n",
        "        x = x_new\n",
        "    return x\n",
        "\n",
        "# 初始化参数\n",
        "x_init = np.random.rand(M)*60+100  # 随机初始化x\n",
        "print(x_init)\n",
        "gamma = 0.9997  # 使用之前计算得到的gamma_tilde\n",
        "alpha = 0.0000001  # 学习率\n",
        "epsilon = 1e-6  # 容忍度\n",
        "\n",
        "# 执行梯度下降算法找到最优x\n",
        "x_opt = gradient_descent(x_init, gamma, alpha, epsilon)\n",
        "\n",
        "x_opt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKQryR5tFEqz",
        "outputId": "a9ce7307-72e4-4ea8-ed16-bc1edd9fca5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[120.5522323  133.76145453 115.88353019]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([120.36190874, 133.60610404, 115.71042473])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To adapt the gradient descent algorithm to use adaptive learning rates, we can employ techniques from popular optimization algorithms such as Adam (Adaptive Moment Estimation) or RMSprop (Root Mean Square Propagation). These algorithms adjust the learning rate for each parameter to improve convergence speed and stability, especially in complex optimization problems.\n",
        "\n",
        "Here, I will demonstrate how to modify the original gradient descent algorithm to use a simplified version of the Adam algorithm. The Adam algorithm combines the concepts of momentum and adaptive learning rates, adjusting the learning rate for each parameter independently."
      ],
      "metadata": {
        "id": "DVFv4vBqdRvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "要将梯度下降算法改造为使用自适应学习率，我们可以采用一些流行的优化算法中的技术，如 Adam（Adaptive Moment Estimation）或 RMSprop（Root Mean Square Propagation）。这些算法通过调整每个参数的学习率来改善收敛速度和稳定性，特别是在复杂的优化问题中。\n",
        "\n",
        "这里，我将展示如何将原先的梯度下降算法修改为使用一个简化版本的 Adam 算法。Adam 算法结合了动量（Momentum）和自适应学习率的概念，对于每个参数独立地调整学习率。"
      ],
      "metadata": {
        "id": "lHX4tIflK09O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This 'adam_gradient_descent' function takes the same parameters: initial 'x', 'gamma', learning rate 'alpha', tolerance 'epsilon', and maximum number of iterations 'max_iter'. It utilizes the core concepts of the Adam algorithm but omits some complexities for the sake of clarity in the example.\n",
        "- $ m^{\\prime} $ and $ v^{\\prime} $ store the first (mean) and second (uncentered variance) moment estimates regarding the gradients, respectively.\n",
        "- 'beta1' and 'beta2' control the exponential decay rates of these moment estimates, which are specific hyperparameters of the Adam algorithm.\n",
        "- '$ m_{\\text{hat}} $' and '$ v_{\\text{hat}} $' are bias-corrected estimates of '$ m $' and '$ v $', used to adjust these estimates in the early stages of the algorithm.\n"
      ],
      "metadata": {
        "id": "df_t1FW0daI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这个 'adam_gradient_descent' 函数接收相同的参数：初始 ' $ x $ '、'gamma 、学习率 'alpha'、容忍度 'epsilon “以及最大迭代次数 'max_iter'。它使用 Adam 算法的核心概念，但省略了一些复杂性以保持示例的清晰性。\n",
        "- $ m^{\\prime} $ 和 $ v^{\\prime} $ 分别存储关于梯度的一阶 (平均) 和二阶 (未中心化的方差) 矩估计。\n",
        "- 'beta1 '和 'beta2'控制这些矩估计的指数衰减率, 这是 Adam 算法特有的超参数。\n",
        "- ' $ m_{-} $hat' 和 ' $ v $ _ hat' 是对 ' $ m $ ' 和 ' $ v $ ' 的偏差校正，用于在算法的早期阶段调整这些估计。"
      ],
      "metadata": {
        "id": "XHZ-x-VzLYTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define assumed parameters\n",
        "M = 10  # Dimensionality, representing the problem size or the number of variables\n",
        "Q_hat = np.random.rand(M, M)  # Randomly generate assumed Q_hat, simulating unknown parameter matrix in real scenarios\n",
        "L1 = np.random.rand(M, M)  # Randomly generate assumed L1, representing some linear transformation or constraint\n",
        "L2 = np.random.rand(M, M)  # Randomly generate assumed L2, also representing some linear transformation or constraint\n",
        "lambda_ = 1  # Assumed lambda, used to adjust the strength of the regularization term\n",
        "\n",
        "# Define the gradient of the g(x, gamma) function\n",
        "def gradient_g(x, gamma):\n",
        "    M1 = L1.T @ L1  # Compute the autocorrelation matrix of L1\n",
        "    M2 = L2.T @ L2  # Compute the autocorrelation matrix of L2\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2  # Combine all terms to form the coefficient matrix of the objective function\n",
        "    grad = 2 * (term.T + term) @ x  # Compute the gradient of the objective function with respect to x\n",
        "    return grad\n",
        "\n",
        "# Define the Adam gradient descent algorithm\n",
        "def adam_gradient_descent(x_init, gamma, alpha, epsilon, max_iter=1000):\n",
        "    x = x_init  # Initialize x\n",
        "    m = np.zeros(x.shape)  # Initialize first moment estimate to 0\n",
        "    v = np.zeros(x.shape)  # Initialize second moment estimate to 0\n",
        "    beta1 = 0.9  # Decay rate for the first moment, used for the first moment estimate\n",
        "    beta2 = 0.999  # Decay rate for the second moment, used for the second moment estimate\n",
        "    eta = alpha  # Learning rate\n",
        "    delta = 1e-8  # Small constant to avoid division by zero\n",
        "\n",
        "    for i in range(1, max_iter + 1):\n",
        "        grad = gradient_g(x, gamma)  # Compute the gradient of x\n",
        "\n",
        "        # Update first and second moment estimates\n",
        "        m = beta1 * m + (1 - beta1) * grad\n",
        "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
        "\n",
        "        # Bias correction for moment estimates\n",
        "        m_hat = m / (1 - beta1 ** i)\n",
        "        v_hat = v / (1 - beta2 ** i)\n",
        "\n",
        "        # Update parameter x\n",
        "        x_new = x - eta * m_hat / (np.sqrt(v_hat) + delta)\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(x_new - x) < epsilon:\n",
        "            break\n",
        "        x = x_new\n",
        "\n",
        "    return x\n",
        "\n",
        "# Initialize parameters and execute the algorithm\n",
        "x_init = np.random.rand(M) * 0.4 + 0.6  # Random initialization for x, ensuring its initial values are within [0.6, 1.0]\n",
        "gamma = 0.9997  # Given gamma value\n",
        "alpha = 0.0001  # Learning rate, a small value to ensure stable convergence\n",
        "epsilon = 1e-6  # Convergence threshold, stop iteration when the difference between consecutive iterations is less than this value\n",
        "max_iter = 1000  # Maximum number of iterations\n",
        "\n",
        "# Optimize x using the Adam algorithm\n",
        "x_opt_adam = adam_gradient_descent(x_init, gamma, alpha, epsilon, max_iter)\n",
        "\n",
        "x_opt_adam\n"
      ],
      "metadata": {
        "id": "I5sUbp6gdoaX",
        "outputId": "549104bb-2d72-4bca-c8b3-5e0b96ec9cfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.55478085, 0.51890734, 0.51502009, 0.6839514 , 0.7875041 ,\n",
              "       0.50809128, 0.7090022 , 0.50895391, 0.65790766, 0.65105125])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 定义假设的参数\n",
        "M = 10  # 维度，表示问题的规模或变量的数量\n",
        "Q_hat = np.random.rand(M, M)  # 随机生成假设的 Q_hat，模拟真实情况下未知的参数矩阵\n",
        "L1 = np.random.rand(M, M)  # 随机生成假设的 L1，代表某种线性变换或约束\n",
        "L2 = np.random.rand(M, M)  # 随机生成假设的 L2，同样代表某种线性变换或约束\n",
        "lambda_ = 1  # 假设的 lambda，用于调节正则化项的强度\n",
        "\n",
        "# 定义 g(x, gamma) 函数的梯度\n",
        "def gradient_g(x, gamma):\n",
        "    M1 = L1.T @ L1  # 计算L1的自相关矩阵\n",
        "    M2 = L2.T @ L2  # 计算L2的自相关矩阵\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2  # 综合所有项，形成目标函数的系数矩阵\n",
        "    grad = 2 * (term.T + term) @ x  # 计算目标函数关于x的梯度\n",
        "    return grad\n",
        "\n",
        "# 定义 Adam 梯度下降算法\n",
        "def adam_gradient_descent(x_init, gamma, alpha, epsilon, max_iter=1000):\n",
        "    x = x_init  # 初始化x\n",
        "    m = np.zeros(x.shape)  # 初始化一阶动量估计为0\n",
        "    v = np.zeros(x.shape)  # 初始化二阶动量估计为0\n",
        "    beta1 = 0.9  # 动量衰减率，用于一阶估计\n",
        "    beta2 = 0.999  # 动量衰减率，用于二阶估计\n",
        "    eta = alpha  # 学习率\n",
        "    delta = 1e-8  # 用于避免除以零的小常数\n",
        "\n",
        "    for i in range(1, max_iter + 1):\n",
        "        grad = gradient_g(x, gamma)  # 计算当前x的梯度\n",
        "\n",
        "        # 更新一阶和二阶动量估计\n",
        "        m = beta1 * m + (1 - beta1) * grad\n",
        "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
        "\n",
        "        # 对动量估计进行偏差校正\n",
        "        m_hat = m / (1 - beta1 ** i)\n",
        "        v_hat = v / (1 - beta2 ** i)\n",
        "\n",
        "        # 更新参数x\n",
        "        x_new = x - eta * m_hat / (np.sqrt(v_hat) + delta)\n",
        "\n",
        "        # 检查是否满足停止准则\n",
        "        if np.linalg.norm(x_new - x) < epsilon:\n",
        "            break\n",
        "        x = x_new\n",
        "\n",
        "    return x\n",
        "\n",
        "# 初始化参数并执行算法\n",
        "x_init = np.random.rand(M) * 0.4 + 0.6  # 随机初始化x，确保其初始值在[0.6, 1.0]之间\n",
        "gamma = 0.9997  # 给定的gamma值\n",
        "alpha = 0.0001  # 学习率，较小值以确保稳定收敛\n",
        "epsilon = 1e-6  # 收敛阈值，当连续两次迭代的差距小于此值时停止迭代\n",
        "max_iter = 1000  # 最大迭代次数\n",
        "\n",
        "# 使用Adam算法优化x\n",
        "x_opt_adam = adam_gradient_descent(x_init, gamma, alpha, epsilon, max_iter)\n",
        "\n",
        "x_opt_adam\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSy_2vlTMZEJ",
        "outputId": "dad90c4c-2d5a-4129-c562-617669f67aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.6419176 , 0.78768227, 0.83342865, 0.83626202, 0.83510915,\n",
              "       0.69743513, 0.66412767, 0.51057612, 0.81938   , 0.54072557])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Coordinate Descent algorithm is an iterative method used for solving optimization problems. It updates only one variable at a time while keeping all other variables fixed. In the provided algorithm, we alternate between optimizing $\\gamma$ and $x$ until the stopping condition is met.\n",
        "\n",
        "To implement this algorithm, we need to define a function for optimizing $\\gamma$ and another function for optimizing $x$. Assuming we already have the expression for $g(x, \\gamma)$ and the corresponding gradient calculation function."
      ],
      "metadata": {
        "id": "Mm_fZZaBEm9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "协同下降（Coordinate Descent）算法是一种用于求解优化问题的迭代方法, 它一次只更新问题中的一个变量, 而保持其他所有变量固定。在给出的算法中, 我们交替地优化 $\\gamma$ 和 $x$, 直到满足停止条件为止。\n",
        "\n",
        "为了实现这个算法，我们需要定义一个用于优化 $\\gamma$ 的函数，以及一个用于优化 $x$ 的函数。假设我们已经有了 $g(x, \\gamma)$ 的表达式和相应的梯度计算函数。"
      ],
      "metadata": {
        "id": "KzweB8tb7_cA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assumed parameters and functions.\n",
        "\n",
        "M = 50  # Dimension\n",
        "Q_hat = np.random.rand(M, M)  # Randomly generated Q_hat\n",
        "L1 = np.random.rand(M, M)  # Randomly generated L1\n",
        "L2 = np.random.rand(M, M)  # Randomly generated L2\n",
        "lambda_ = 1  # Assumed lambda\n",
        "alpha = 0.0001  # Initial search interval lower bound\n",
        "beta = 2  # Initial search interval upper bound\n",
        "delta = 0.001  # Accuracy requirement\n",
        "\n",
        "# Define the g(x, gamma) function\n",
        "def g(x, gamma):\n",
        "    M1 = L1.T @ L1\n",
        "    M2 = L2.T @ L2\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2\n",
        "    return x.T @ term @ x\n",
        "\n",
        "# Define the function to optimize x given gamma\n",
        "def optimize_x(gamma, epsilon=1e-6, max_iter=10000):\n",
        "    x_init = np.random.rand(M) * 0.4 + 0.6\n",
        "    # print(\"x_init: \" + str(x_init))\n",
        "    # print()\n",
        "    return adam_gradient_descent(x_init, gamma, alpha, epsilon, max_iter)\n",
        "\n",
        "# Define the function to optimize gamma given x\n",
        "def optimize_gamma(x, epsilon=1e-6, max_iter=10000):\n",
        "    # Calculate the optimal gamma based on the provided formula\n",
        "    numerator = np.linalg.norm(L2 @ x)\n",
        "    denominator = np.linalg.norm(L1 @ x)\n",
        "    return numerator / denominator if denominator != 0 else np.inf\n",
        "\n",
        "# Implement the coordinate descent algorithm\n",
        "def coordinate_descent(Q_hat, L1, L2, lambda_, gamma_0, delta, epsilon=1e-6, max_iter=1000):\n",
        "    assert gamma_0 > 0, \"gamma_0 must be positive\"\n",
        "    r = np.inf\n",
        "    tilde_gamma = gamma_0\n",
        "    tilde_x = optimize_x(tilde_gamma, epsilon, max_iter)\n",
        "    k = 0\n",
        "    while r - g(tilde_x, tilde_gamma) > delta and not np.isinf(tilde_gamma) and tilde_gamma != 0:\n",
        "        r = g(tilde_x, tilde_gamma)\n",
        "        # print(\"r - g(tilde_x, tilde_gamma): \"+ str(r - g(tilde_x, tilde_gamma)))\n",
        "        # print()\n",
        "        tilde_x = optimize_x(tilde_gamma, epsilon, max_iter)\n",
        "        # print(\"tilde_x:\" + str(tilde_x))\n",
        "        # print()\n",
        "        tilde_gamma = optimize_gamma(tilde_x, epsilon, max_iter)\n",
        "        # print(\"tilde_gamma: \" + str(tilde_gamma))\n",
        "        # print(\"-------------------------------\")\n",
        "        k+=1\n",
        "    print(\"iterarion number: \" + str(k))# Print the current iteration number\n",
        "    return tilde_x\n",
        "\n",
        "# Call the coordinate_descent function\n",
        "tilde_x = coordinate_descent(Q_hat, L1, L2, lambda_, gamma_0=0.9997, delta=1e-6)\n",
        "\n",
        "print(\"Optimized x:\", tilde_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gVpvoidEtTu",
        "outputId": "10c4e8ab-f452-4ef0-982f-20579dffc00b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iterarion number: 1\n",
            "Optimized x: [0.60565818 0.83783217 0.53165555 0.77711726 0.60196622 0.88799899\n",
            " 0.76913247 0.81074708 0.515069   0.58218003 0.59329502 0.88521529\n",
            " 0.87788958 0.84409769 0.85026126 0.8140046  0.58946646 0.54648259\n",
            " 0.53617678 0.87352587 0.67673566 0.83937484 0.65062879 0.66316057\n",
            " 0.64149921 0.5418017  0.87515111 0.77197786 0.75259372 0.89102656\n",
            " 0.7179639  0.89627815 0.58236388 0.79632987 0.71431533 0.66567003\n",
            " 0.53725826 0.62310462 0.83513978 0.72929133 0.87752745 0.62144847\n",
            " 0.76635149 0.88874444 0.82426523 0.71791422 0.52807274 0.56359896\n",
            " 0.65166574 0.8612838 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 假设的参数和函数.\n",
        "\n",
        "M = 50  # 维度\n",
        "Q_hat = np.random.rand(M, M)  # 随机生成的 Q_hat\n",
        "L1 = np.random.rand(M, M)  # 随机生成的 L1\n",
        "L2 = np.random.rand(M, M)  # 随机生成的 L2\n",
        "lambda_ = 1  # 假设的 lambda\n",
        "alpha = 0.0001  # 初始搜索区间下限\n",
        "beta = 2  # 初始搜索区间上限\n",
        "delta = 0.001  # 精度要求\n",
        "# x_init = np.random.rand(M) * 0.4 + 0.6  # 随机初始化x，确保其初始值在[0.6, 1.0]之间\n",
        "\n",
        "# 定义 g(x, gamma) 函数\n",
        "def g(x, gamma):\n",
        "    M1 = L1.T @ L1\n",
        "    M2 = L2.T @ L2\n",
        "    term = Q_hat + lambda_ * (gamma * M1 + M2 / gamma) / 2\n",
        "    return x.T @ term @ x\n",
        "\n",
        "# 定义优化x给定gamma的函数\n",
        "def optimize_x(gamma, epsilon=1e-6, max_iter=10000):\n",
        "    x_init = np.random.rand(M) * 0.4 + 0.6\n",
        "    # print(\"x_init: \" + str(x_init))\n",
        "    # print()\n",
        "    return adam_gradient_descent(x_init, gamma, alpha, epsilon, max_iter)\n",
        "\n",
        "# 定义优化gamma给定x的函数\n",
        "def optimize_gamma(x, epsilon=1e-6, max_iter=10000):\n",
        "    # 根据提供的公式计算最优gamma\n",
        "    numerator = np.linalg.norm(L2 @ x)\n",
        "    denominator = np.linalg.norm(L1 @ x)\n",
        "    return numerator / denominator if denominator != 0 else np.inf\n",
        "\n",
        "# 实现坐标下降算法\n",
        "def coordinate_descent(Q_hat, L1, L2, lambda_, gamma_0, delta, epsilon=1e-6, max_iter=1000):\n",
        "    assert gamma_0 > 0, \"gamma_0 must be positive\"\n",
        "    r = np.inf\n",
        "    tilde_gamma = gamma_0\n",
        "    tilde_x = optimize_x(tilde_gamma, epsilon, max_iter)\n",
        "    k = 0\n",
        "    while r - g(tilde_x, tilde_gamma) > delta and not np.isinf(tilde_gamma) and tilde_gamma != 0:\n",
        "        r = g(tilde_x, tilde_gamma)\n",
        "        # print(\"r - g(tilde_x, tilde_gamma): \"+ str(r - g(tilde_x, tilde_gamma)))\n",
        "        # print()\n",
        "        tilde_x = optimize_x(tilde_gamma, epsilon, max_iter)\n",
        "        # print(\"tilde_x:\" + str(tilde_x))\n",
        "        # print()\n",
        "        tilde_gamma = optimize_gamma(tilde_x, epsilon, max_iter)\n",
        "        # print(\"tilde_gamma: \" + str(tilde_gamma))\n",
        "        # print(\"-------------------------------\")\n",
        "        k+=1\n",
        "    print(\"iterarion number: \" + str(k))# 打印当前迭代次数\n",
        "    return tilde_x\n",
        "\n",
        "# 调用coordinate_descent函数\n",
        "tilde_x = coordinate_descent(Q_hat, L1, L2, lambda_, gamma_0=0.9997, delta=1e-6)\n",
        "\n",
        "print(\"Optimized x:\", tilde_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFkEqQEy6IRi",
        "outputId": "c2618e9e-eb06-4c97-ed08-8dff0d6da5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iterarion number: 2\n",
            "Optimized x: [0.69195959 0.50806386 0.66686875 0.89098634 0.77738243 0.89109282\n",
            " 0.70517478 0.51696428 0.60327885 0.77361593 0.77010751 0.62441245\n",
            " 0.55873733 0.69543197 0.51831318 0.53820073 0.88777158 0.71668953\n",
            " 0.62614358 0.76525257 0.59472833 0.81796139 0.60351285 0.81120636\n",
            " 0.73411257 0.61359801 0.7930147  0.82629618 0.81409763 0.83354057\n",
            " 0.89490024 0.74590423 0.75353898 0.70750824 0.52607979 0.72457695\n",
            " 0.61850041 0.71184033 0.81532576 0.89369101 0.73527826 0.62708038\n",
            " 0.87217864 0.68908371 0.70503849 0.59775077 0.87467072 0.8607943\n",
            " 0.64984926 0.79140835]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "StM941cn_PRi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}